[Global]
epoch=30
batch_size=30
grad_norm=1.0
early_stopping={ metric='f1' }


[[Fields]]
class = "text"
    [Fields.init]
    name = "text"
    [Fields.init.tokenizer]
    path="data/electra_base"

[[Fields]]
class = "sequence"
    [Fields.init]
    name = 'tag'
    unk = false
    is_target = true
    pad_bias = true

[Dataset]
class = "line"
path="data/ltp-data/ner"
train="train.txt"
validation="valid.txt"
test="test.txt"
  [Dataset.init]
  split='/.+#'

[[Metrics]]
class = "Sequence"
    [Metrics.init]
    id2label='tag'
    no_suffix=true

[Loss]
class = "CrossEntropyLoss"
    [Loss.init]
    ignore_index=-1

[Optimizer]
class = "BertAdamW"
    [Optimizer.init]
    lr = 1e-4
    bert_lr = 1e-4
    weight_decay = 0

[Scheduler]
class = "LinearLRW"
type = 'step'
warmup_proportion=0.002

[Predictor]
class = "simple"

[Checkpoint]
directory="checkpoint/ltp_base/ner_rela"
plugins=['best','restore']
best={ metric='f1' }

[Model]
class = "SimpleTagging"

    [Model.init]
    label_num=13
    word_base=true
    pretrained="data/electra_base"
    decoder="RelativeTransformer"
    [Model.init.RelativeTransformer]
    num_heads=4
    num_layers=2
    hidden_size=256
